{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e078dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.chdir('Transformer-MM-Explainability')\n",
    "os.chdir('ConvT-explain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0dc5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets\n",
    "\n",
    "# from DETR.datasets.coco import *\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import DETR.util.misc as utils\n",
    "\n",
    "from DETR.models import build_model\n",
    "from DETR.modules.ExplanationGenerator import Generator\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "477ccecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DETR.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b5d5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "        # COCO classes\n",
    "CLASSES = [\n",
    "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
    "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
    "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
    "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "    'toothbrush'\n",
    "]\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "\n",
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "def plot_results(pil_img, prob, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        cl = p.argmax()\n",
    "        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9952ac5",
   "metadata": {},
   "source": [
    "### Evaluate function from Analysis across heads.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a3208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, gen, im, device, image_id=None, show_all_layers=False, show_raw_attn=False):\n",
    "\n",
    "    # 평균-분산 정규화 (train dataset의 통계량을 (test) input image에 사용\n",
    "    img=transform(im).unsqueeze(0).to(device)\n",
    "    \n",
    "    # model 통과\n",
    "\n",
    "    outputs =model(img)\n",
    "    \n",
    "    # 정확도 70% 이상의 예측만 사용\n",
    "    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1] # background 제외\n",
    "    keep = probas.max(-1).values > 0.7\n",
    "    \n",
    "    if keep.nonzero().shape[0] <=1 : # detect된 object\n",
    "        return\n",
    "    \n",
    "    #* 한 개는 evalute 할 필요가 없나요\n",
    "    \n",
    "    # 원래 cuda에 적재되어있던 좌표들\n",
    "    outputs['pred_boxes'] = outputs['pred_boxes'].cpu()\n",
    "    \n",
    "    # [0,1]의 상대 좌표를 원래의 좌표로 복구\n",
    "    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
    "\n",
    "\n",
    "    #attention weight 저장\n",
    "\n",
    "    conv_features_in, enc_attn_in, dec_attn_in = [], [], []\n",
    "    conv_features_out, enc_attn_out, dec_attn_out = [], [], []\n",
    "\n",
    "    # 이 때, output[0] : [token, 1, hidden_dim] --> 토큰\n",
    "    # output[1] : [1, token, token] --> 가중치\n",
    "    hooks = [\n",
    "#         # real bacbkone (backbone[-1] : positional embeddings)\n",
    "#         model.backbone[-2].register_forward_hook(\n",
    "#         lambda self, input, output: conv_features.append(output)\n",
    "#         ),\n",
    "        \n",
    "        #transformer encoder 내 마지막 layer의 self attention 층\n",
    "#         model.transformer.encoder.layers[-1].self_attn.register_forward_hook(\n",
    "#         lambda self, input, output: enc_attn_weights.append(output)\n",
    "#                 ),\n",
    "        \n",
    "#         # transformer decoder 내 마지막 layer의 multihead_attn 층\n",
    "#         model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(\n",
    "#         lambda self, input, output: dec_attn_weights.append(output)\n",
    "#         ),\n",
    "        \n",
    "#         model.transformer.encoder.layers[-1].self_attn.register_forward_hook(\n",
    "#         lambda self, input, output: test_weights.append(output)\n",
    "#         )\n",
    "    \n",
    "    ]\n",
    "       \n",
    "    ## Cnv\n",
    "    # in\n",
    "    for layer_name in model.backbone[-2].body:\n",
    "        hook=model.backbone[-2].body[layer_name].register_forward_hook(\n",
    "        lambda self, input, output : conv_features_in.append(input)\n",
    "        )\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    hook=model.backbone[-1].register_forward_hook(\n",
    "    lambda self, input, output : conv_features_in.append(input))\n",
    "    #out\n",
    "    for layer_name in model.backbone[-2].body:\n",
    "        hook=model.backbone[-2].body[layer_name].register_forward_hook(\n",
    "        lambda self, input, output : conv_features_out.append(output)\n",
    "        )\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    hook=model.backbone[-1].register_forward_hook(\n",
    "    lambda self, input, output : conv_features_out.append(output))\n",
    "            \n",
    "    # transformer encoder 내 모든 layer의 output 저장\n",
    "    # default : (enc_layer = 6)\n",
    "    \n",
    "    ## encoder\n",
    "    # in\n",
    "    for layer in model.transformer.encoder.layers:\n",
    "        hook=layer.self_attn.register_forward_hook(\n",
    "        lambda self, input, output : enc_attn_in.append(input)\n",
    "        )\n",
    "        hooks.append(hook)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # out    \n",
    "    for layer in model.transformer.encoder.layers:\n",
    "        hook=layer.self_attn.register_forward_hook(\n",
    "        lambda self, input, output : enc_attn_out.append(output)\n",
    "        )\n",
    "        hooks.append(hook)\n",
    "        \n",
    "        \n",
    "    ## decoder\n",
    "    # in\n",
    "    for layer in model.transformer.decoder.layers:\n",
    "        hook=layer.self_attn.register_forward_hook(\n",
    "        lambda self, input, output : dec_attn_in.append(input)\n",
    "        )\n",
    "        hooks.append(hook)\n",
    "    # out\n",
    "    for layer in model.transformer.decoder.layers:\n",
    "        hook=layer.self_attn.register_forward_hook(\n",
    "        lambda self, input, output : dec_attn_out.append(output)\n",
    "        )\n",
    "        hooks.append(hook)\n",
    "    # 모델 통과(및 저장)\n",
    "    model(img)\n",
    "    \n",
    "#     return enc_attn_weights\n",
    "    # hook 제거\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    # 리스트는 필요 없다.\n",
    "    # 우선 위의 for문을 통해 encoder layer의 모든 가중치를 저장은 해놨으나,\n",
    "    # 여기서는 encoder의 마지막 layer만을 사용하자.\n",
    "#     conv_features = conv_features[0] # feature-map\n",
    "#     enc_attn_weights = enc_attn_weights[-1] # 마지막 encoder layer의 가중치만 사용 (256개의 값)\n",
    "#     dec_attn_weights = dec_attn_weights[0] # 마지막 decoder layer의 가중치만 사용 (256개의 값)\n",
    "    \n",
    "    #  get the shape of feature map\n",
    "#     return conv_features\n",
    "    h, w = conv_features_out[-1].shape[-2:] # Nested tensor -> tensors\n",
    "# #     img_np = np.array(im).astype(np.float)\n",
    "    if not show_all_layers == True:\n",
    "        fig, axs = plt.subplots(ncols=len(bboxes_scaled), nrows=2, figsize=(22,7))\n",
    "    else:\n",
    "        n_layers=len(model.transformer.encoder.layers)\n",
    "        if not show_raw_attn:\n",
    "            fig, axs = plt.subplots(ncols=len(bboxes_scaled), nrows=n_layers+1, figsize=(22, 4*n_layers))\n",
    "        else:\n",
    "            fig, axs = plt.subplots(ncols=len(bboxes_scaled), nrows=model.transformer.nhead+1,\n",
    "                                    figsize=(22, 4*model.transformer.nhead))\n",
    "    # object queries는 100차원(default)이기 때문에 그 중에 \n",
    "    # 0.7(default) 이상의 신뢰도를 보이는 query만을 사용해야 한다. \n",
    "    \n",
    "    for idx, ax_i, (xmin, ymin, xmax, ymax) in zip(keep.nonzero(), axs.T, bboxes_scaled):\n",
    "        \n",
    "        ax = ax_i[0]\n",
    "        ax.imshow(im)\n",
    "        ax.add_patch(plt.Rectangle((xmin.detach(), ymin.detach()), \n",
    "                                  xmax.detach() - xmin.detach(),\n",
    "                                   ymax.detach() - ymin.detach(), \n",
    "                                   fill=False, color='blue', linewidth=3))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(CLASSES[probas[idx].argmax()])\n",
    "        \n",
    "      \n",
    "        \n",
    "        if not show_all_layers == True:\n",
    "            ax = ax_i[1]\n",
    "                            \n",
    "            \n",
    "            cam = gen.generate_ours(img, idx, use_lrp=False)\n",
    "            cam = (cam - cam.min()) / (cam.max() - cam.min()) # 점수 정규화\n",
    "            cmap = plt.cm.get_cmap('Blues').reversed()\n",
    "\n",
    "            ax.imshow(cam.view(h, w).data.cpu().numpy(), cmap=cmap)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f'query id: {idx.item()}')\n",
    "        else:\n",
    "            \n",
    "            if not show_raw_attn:    \n",
    "                cams = gen.generate_ours(img, idx, use_lrp=False, use_all_layers=True)\n",
    "            else:\n",
    "                cams = gen.generate_raw_attn(img, idx, use_all_layers=True)\n",
    "            \n",
    "            num_layer=n_layers\n",
    "            if show_raw_attn:\n",
    "                num_layer=model.transformer.nhead\n",
    "            for n, cam in zip(range(num_layer), cams):\n",
    "                ax = ax_i[1+n]\n",
    "                cam = (cam - cam.min()) / (cam.max() - cam.min()) # 점수 정규화\n",
    "                cmap = plt.cm.get_cmap('Blues').reversed()\n",
    "\n",
    "                ax.imshow(cam.view(h, w).data.cpu().numpy(), cmap=cmap)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'query id: {idx.item()}, layer:{n}', size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b23f8ea",
   "metadata": {},
   "source": [
    "### Expalnation function from *Analysis across heads.ipynb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e044e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, model):\n",
    "        print('### Generator.init()')\n",
    "        self.model = model\n",
    "        self.model.eval() # evaluate 시 dropout, batchnorm 등은 사용하지 않는다.\n",
    "        self.use_all_layers=False\n",
    "        self.use_all_layers=False\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "#         print('### Generator.forward(input_ids, attention_mask)')\n",
    "        return self.model(input_ids, attention_mask)\n",
    "\n",
    "    def generate_transformer_att(self, img, target_index, index=None):\n",
    "        outputs = self.model(img)\n",
    "        kwargs = {\"alpha\": 1,\n",
    "                  \"target_index\": target_index}\n",
    "\n",
    "        if index == None:\n",
    "            index = outputs['pred_logits'][0, target_index, :-1].max(1)[1]\n",
    "\n",
    "        kwargs[\"target_class\"] = index\n",
    "\n",
    "        one_hot = torch.zeros_like(outputs['pred_logits']).to(outputs['pred_logits'].device)\n",
    "        one_hot[0, target_index, index] = 1\n",
    "        one_hot_vector = one_hot.clone().detach()\n",
    "        one_hot.requires_grad_(True)\n",
    "        one_hot = torch.sum(one_hot.cuda() * outputs['pred_logits'])\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        one_hot.backward(retain_graph=True)\n",
    "\n",
    "        self.model.relprop(one_hot_vector, **kwargs)\n",
    "\n",
    "        decoder_blocks = self.model.transformer.decoder.layers\n",
    "        encoder_blocks = self.model.transformer.encoder.layers\n",
    "\n",
    "        # initialize relevancy matrices\n",
    "        image_bboxes = encoder_blocks[0].self_attn.get_attn().shape[-1]\n",
    "        queries_num = decoder_blocks[0].self_attn.get_attn().shape[-1]\n",
    "\n",
    "        # image self attention matrix\n",
    "        self.R_i_i = torch.eye(image_bboxes, image_bboxes).to(encoder_blocks[0].self_attn.get_attn().device)\n",
    "        # queries self attention matrix\n",
    "        self.R_q_q = torch.eye(queries_num, queries_num).to(encoder_blocks[0].self_attn.get_attn().device)\n",
    "        # impact of image boxes on queries\n",
    "        self.R_q_i = torch.zeros(queries_num, image_bboxes).to(encoder_blocks[0].self_attn.get_attn().device)\n",
    "\n",
    "        # R_q_i generated from last layer\n",
    "        decoder_last = decoder_blocks[-1]\n",
    "        cam_q_i = decoder_last.multihead_attn.get_attn_cam().detach()\n",
    "        grad_q_i = decoder_last.multihead_attn.get_attn_gradients().detach()\n",
    "        cam_q_i = avg_heads(cam_q_i, grad_q_i)\n",
    "        self.R_q_i = cam_q_i\n",
    "        aggregated = self.R_q_i.unsqueeze_(0)\n",
    "\n",
    "        aggregated = aggregated[:, target_index, :].unsqueeze_(0)\n",
    "        return aggregated\n",
    "\n",
    "    def handle_self_attention_image(self, blocks):\n",
    "        for blk in blocks:\n",
    "            grad = blk.self_attn.get_attn_gradients().detach() # [8 x wh x wh]의 gradient(당연히 가중치랑 같음)\n",
    "            if self.use_lrp: # 타당성 전파 시\n",
    "                cam = blk.self_attn.get_attn_cam().detach() # [8 x wh x wh]의 cam(*)\n",
    "            else:\n",
    "                cam = blk.self_attn.get_attn().detach()\n",
    "            cam = avg_heads(cam, grad)  # [wh x wh]의 averaged cam\n",
    "            \n",
    "            \n",
    "            self.R_i_i += torch.matmul(cam, self.R_i_i)# [wh x wh] X [wh x wh]\n",
    "            \n",
    "            if self.use_all_layers == True:\n",
    "                self.R_i_i_all.append(self.R_i_i.detach().clone())\n",
    "                \n",
    "    def handle_co_attn_self_query(self, block):\n",
    "        grad = block.self_attn.get_attn_gradients().detach()\n",
    "        if self.use_lrp:\n",
    "            cam = block.self_attn.get_attn_cam().detach()\n",
    "        else:\n",
    "            cam = block.self_attn.get_attn().detach()\n",
    "        cam = avg_heads(cam, grad)\n",
    "        R_q_q_add, R_q_i_add = apply_self_attention_rules(self.R_q_q, self.R_q_i, cam) # 식 (6)(7), 행렬곱\n",
    "        self.R_q_q += R_q_q_add\n",
    "        self.R_q_i += R_q_i_add\n",
    "        \n",
    "        if self.use_all_layers == True:\n",
    "            self.R_q_q_all.append(self.R_q_q.detach().clone())\n",
    "\n",
    "    def handle_co_attn_query(self, block):\n",
    "        if self.use_lrp:\n",
    "            cam_q_i = block.multihead_attn.get_attn_cam().detach()  # multihead\n",
    "        else:\n",
    "            cam_q_i = block.multihead_attn.get_attn().detach()\n",
    "        grad_q_i = block.multihead_attn.get_attn_gradients().detach()\n",
    "        cam_q_i = avg_heads(cam_q_i, grad_q_i) # = [100 x wh]\n",
    "        self.R_q_i += apply_mm_attention_rules(self.R_q_q, self.R_i_i, cam_q_i, # 식 (10), 행렬곱(R_ii x cam xi R_q_q)\n",
    "                                               apply_normalization=self.normalize_self_attention, # R_qq, R_ii 정규화\n",
    "                                               apply_self_in_rule_10=self.apply_self_in_rule_10) # R_sq 대신 cam_sq (*)\n",
    "        if self.use_all_layers == True:\n",
    "            self.R_q_i_all.append(self.R_q_i.detach().clone())\n",
    "    def generate_ours(self, img, target_index, index=None, use_lrp=True,\n",
    "                     normalize_self_attention=True, apply_self_in_rule_10=True, use_all_layers=False):\n",
    "        \n",
    "\n",
    "        self.use_lrp = use_lrp\n",
    "        self.normalize_self_attention = normalize_self_attention\n",
    "        self.apply_self_in_rule_10 = apply_self_in_rule_10\n",
    "        self.use_all_layers = use_all_layers\n",
    "\n",
    "        outputs = self.model(img)\n",
    "        outputs = outputs['pred_logits']\n",
    "        \n",
    "\n",
    "        kwargs = {\"alpha\": 1, \n",
    "                 \"target_index\": target_index}\n",
    "        \n",
    "        if index == None:\n",
    "            index = outputs[0, target_index, :-1].max(1)[1]\n",
    "\n",
    "        kwargs[\"target_class\"] = index\n",
    "        \n",
    "        one_hot = torch.zeros_like(outputs).to(outputs.device)\n",
    "        one_hot[0, target_index, index] = 1 # [1, 100, 92] 차원으로 된 원핫벡터\n",
    "        one_hot_vector = one_hot # 나중에 타당성 전파하기 위함\n",
    "\n",
    "        one_hot.requires_grad_(True) # 그래디언트 추적 # 복사 후 원본은 다시 autograd가 추적해야 한다.\n",
    "        one_hot = torch.sum(one_hot.cuda() * outputs)\n",
    "        \n",
    "       \n",
    "        \n",
    "        self.model.zero_grad() # 모델 내 그래디언트를 0으로 초기화\n",
    "\n",
    "        one_hot.backward(retain_graph=True) # backward를 하는 동안에 중간 가중치들은 보존한다.\n",
    "        \n",
    "        \n",
    "        if use_lrp:\n",
    "            return \n",
    "\n",
    "            self.model.relprop(one_hot_vector, **kwargs)\n",
    "\n",
    "        decoder_blocks = self.model.transformer.decoder.layers\n",
    "        encoder_blocks = self.model.transformer.encoder.layers\n",
    "\n",
    "        # 픽셀 개수\n",
    "        image_bboxes = encoder_blocks[0].self_attn.get_attn().shape[-1] # wh in (8, wh, wh)\n",
    "        # object 개수\n",
    "        queries_num = decoder_blocks[0].self_attn.get_attn().shape[-1] # 100 in (8,100,100)\n",
    "        # 타당성 행렬(Relevancy matrices) 초기화\n",
    "        # 또한, 계산 자체가 attention weights랑 행해지기 때문에 device 맞춰주기\n",
    "        # image self attention matrix - (wh x wh) 크기의 Identity 행렬\n",
    "        self.R_i_i = torch.eye(image_bboxes, image_bboxes).to(encoder_blocks[0].self_attn.get_attn().device)\n",
    "        \n",
    "        # queries self attention matrix\n",
    "        self.R_q_q = torch.eye(queries_num, queries_num).to(encoder_blocks[0].self_attn.get_attn().device)\n",
    "        # image --> queries의 영향 ( (100 x wh) 크기 행렬)\n",
    "        self.R_q_i = torch.zeros(queries_num, image_bboxes).to(encoder_blocks[0].self_attn.get_attn().device)\n",
    "        \n",
    "        if self.use_all_layers == True:\n",
    "            self.R_i_i_all = []\n",
    "            self.R_q_q_all = []\n",
    "            self.R_q_i_all = []\n",
    "        \n",
    "         # encoder 내에서 image에 대한 self-attention\n",
    "\n",
    "        self.handle_self_attention_image(encoder_blocks)\n",
    "    \n",
    "        # decoder 내에서 queries에 대한 self-attention + Multi-modal attention\n",
    "\n",
    "        for idx, blk in enumerate(decoder_blocks):\n",
    "            # decoder self attention\n",
    "            self.handle_co_attn_self_query(blk)\n",
    "\n",
    "            # encoder decoder attention\n",
    "            self.handle_co_attn_query(blk)\n",
    "      \n",
    "        \n",
    "        if not self.use_all_layers:\n",
    "            aggregated = self.R_q_i.unsqueeze_(0)\n",
    "            aggregated = aggregated[:,target_index, :].unsqueeze_(0).detach()\n",
    "            # 결과적으로 위 타겟에 대한 [1, 1, 1, wh] 크기의 cam(if not use_all_layers)\n",
    "            # 굳이 이렇게 만드는 이유는 아마 [layer, target_index, ...] 를 위해?            \n",
    "        else:\n",
    "            # [6, 1, 100, wh]\n",
    "\n",
    "            aggregated = torch.stack(self.R_q_i_all).unsqueeze_(1)\n",
    "            # [6, 1, 1, 1, wh]\n",
    "            aggregated = aggregated[:, :, target_index, :].unsqueeze_(1).detach()\n",
    "\n",
    "        \n",
    "        \n",
    "        return aggregated \n",
    "\n",
    "    \n",
    "    def generate_raw_attn(self, img, target_index, use_all_layers=False):\n",
    "        outputs = self.model(img)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # get cross attn cam from last decoder layer\n",
    "        cam_q_i = self.model.transformer.decoder.layers[-1].multihead_attn.get_attn().detach()\n",
    "        cam_q_i = cam_q_i.reshape(-1, cam_q_i.shape[-2], cam_q_i.shape[-1])\n",
    "        #         \n",
    "        if use_all_layers:\n",
    "            self.R_q_i_all = cam_q_i\n",
    "            aggregated = self.R_q_i_all.unsqueeze_(1)\n",
    "            aggregated = aggregated[:, :,  target_index, :].unsqueeze_(1)\n",
    "        else : \n",
    "            cam_q_i = cam_q_i.mean(dim=0)\n",
    "            self.R_q_i = cam_q_i\n",
    "            aggregated = self.R_q_i.unsqueeze_(0)\n",
    "\n",
    "            aggregated = aggregated[:, target_index, :].unsqueeze_(0)\n",
    "\n",
    "\n",
    "        return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6308c8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c1ddc54",
   "metadata": {},
   "source": [
    "## A. Analysis across feature map level\n",
    "> 즉, Backbone model의 Feature maps에 따른 시각화 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f08722dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['transformer.encoder.layers.0.self_attn.in_proj_weight', 'transformer.encoder.layers.0.self_attn.in_proj_bias', 'transformer.encoder.layers.1.self_attn.in_proj_weight', 'transformer.encoder.layers.1.self_attn.in_proj_bias', 'transformer.encoder.layers.2.self_attn.in_proj_weight', 'transformer.encoder.layers.2.self_attn.in_proj_bias', 'transformer.encoder.layers.3.self_attn.in_proj_weight', 'transformer.encoder.layers.3.self_attn.in_proj_bias', 'transformer.encoder.layers.4.self_attn.in_proj_weight', 'transformer.encoder.layers.4.self_attn.in_proj_bias', 'transformer.encoder.layers.5.self_attn.in_proj_weight', 'transformer.encoder.layers.5.self_attn.in_proj_bias', 'transformer.decoder.layers.0.self_attn.in_proj_weight', 'transformer.decoder.layers.0.self_attn.in_proj_bias', 'transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'transformer.decoder.layers.1.self_attn.in_proj_weight', 'transformer.decoder.layers.1.self_attn.in_proj_bias', 'transformer.decoder.layers.1.multihead_attn.in_proj_weight', 'transformer.decoder.layers.1.multihead_attn.in_proj_bias', 'transformer.decoder.layers.2.self_attn.in_proj_weight', 'transformer.decoder.layers.2.self_attn.in_proj_bias', 'transformer.decoder.layers.2.multihead_attn.in_proj_weight', 'transformer.decoder.layers.2.multihead_attn.in_proj_bias', 'transformer.decoder.layers.3.self_attn.in_proj_weight', 'transformer.decoder.layers.3.self_attn.in_proj_bias', 'transformer.decoder.layers.3.multihead_attn.in_proj_weight', 'transformer.decoder.layers.3.multihead_attn.in_proj_bias', 'transformer.decoder.layers.4.self_attn.in_proj_weight', 'transformer.decoder.layers.4.self_attn.in_proj_bias', 'transformer.decoder.layers.4.multihead_attn.in_proj_weight', 'transformer.decoder.layers.4.multihead_attn.in_proj_bias', 'transformer.decoder.layers.5.self_attn.in_proj_weight', 'transformer.decoder.layers.5.self_attn.in_proj_bias', 'transformer.decoder.layers.5.multihead_attn.in_proj_weight', 'transformer.decoder.layers.5.multihead_attn.in_proj_bias'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device='cuda'\n",
    "args = Namespace(aux_loss=True, backbone='resnet50', batch_size=1, bbox_loss_coef=5, clip_max_norm=0.1, coco_panoptic_path=None, coco_path=None, dataset_file='coco', dec_layers=6, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dropout=0.1, enc_layers=6, eos_coef=0.1, epochs=300, eval=False, frozen_weights=None, giou_loss_coef=2, hidden_dim=256, lr=0.0001, lr_backbone=1e-05, lr_drop=200, mask_loss_coef=1, masks=False, nheads=8, num_queries=100, num_workers=2, output_dir='', position_embedding='sine', pre_norm=False, remove_difficult=False, resume='https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth', seed=42, set_cost_bbox=5, set_cost_class=1, set_cost_giou=2, start_epoch=0, weight_decay=0.0001, world_size=1)\n",
    "\n",
    "model, criterion, postprocessors = build_model(args)\n",
    "model.to(device)\n",
    "\n",
    "checkpoint=torch.hub.load_state_dict_from_url(\n",
    "args.resume, map_location='cpu', check_hash=True)\n",
    "\n",
    "model.load_state_dict(checkpoint['model'], strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2862f8f",
   "metadata": {},
   "source": [
    "### 마지막 layer 삭제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bdc81e",
   "metadata": {},
   "source": [
    "> 이는 기본적으로 너무 힘들 듯 하다(https://velog.io/@sjinu/Various-Attempts). \n",
    "\n",
    "1. model.input_proj : 2048 -> 256 로 학습됨(아마 DETR에서도)  \n",
    "그래서, 이를 1024 -> 256으로 대체할 수 없다면 사용이 조금 곤란하다. (DETR 자체에서 성능이 높아지게끔 학습시켜놓은거라..)  \n",
    "- 그냥 대체용으로 Resnet50에서 각 layer의 역할 정도만 살펴보자. \n",
    "- 또한, 역전파 과정에서 정보를 전달하는 게 최선일듯 하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54882109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model.backbone[0].body.layer4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8683cee",
   "metadata": {},
   "source": [
    "### Backbone : Joiner - Resnet + positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "15fc866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models._utils import IntermediateLayerGetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1ba6bf31",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 8 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2209367/1829345870.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mIntermediateLayearGetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mto_layer3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/jsp_xod/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 8 were given"
     ]
    }
   ],
   "source": [
    "IntermediateLayearGetter(*to_layer3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jsp_xod",
   "language": "python",
   "name": "jsp_xod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
